@[TOC](这里写目录标题)

# 基础 

## db基础

**三大范式**

- 第一范式，表中的所有字段值都是不可分解的原子值。
- 第二范式，表中的每一列都和主键相关，表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。
- 第三范式，表中的每一列数据都和主键直接相关，而不能间接相关。

**主键、外键**

- 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值。
- 外键：在一个表中存在的另一个表的主键称此表的外键。

**主键设计原则**

- 满足唯一和非空的约束；
- 主键字段应尽可能地小；
- 主键字段值基本不会被修改；
- 优先考虑自增字段，或查询最频繁的字段。

**SQL分类**

- 数据定义语言DDL（Data Ddefinition Language）CREATE，DROP，ALTER。包括操作表结构，视图和索引等。

- 数据查询语言DQL（Data Query Language）SELECT

- 数据操纵语言DML（Data Manipulation Language）INSERT，UPDATE，DELETE

- 数据控制功能DCL（Data Control Language）GRANT，REVOKE，COMMIT，ROLLBACK。对数据库安全性完整性等有操作的，包括权限控制等。

  drop、delete与truncate，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。

**SQL约束**

- NOT NULL: 用于控制字段的内容一定不能为空NULL。
- UNIQUE: 字段内容不能重复，一个表允许有多个Unique 约束。
- PRIMARY KEY: 控制字段内容不能重复且不能为空，但它在一个表只允许出现一个。
- CHECK: 用于控制字段的值范围。
- FOREIGN KEY: 外键。使用外键的好处是可以使得两张表存在关联，保证数据的一致性和实现一些级联操作。外键对于主从表行为分类如下：

1、NO ACTION(默认行为) 

删除：从表数据记录不存在时，主表数据记录才可以删除。删除从表数据，主表数据不变 

更新：从表记录数据不存在时，主表数据才可以更新（主表的其他字段能更新，外键关联的字段不能更新）。当更新从表数据，主表数据不变

2、CASCADE（级联） 

删除：删除主表数据时自动删除从表数据。删除从表数据，主表数据不变 

更新：更新主表数据时自动更新从表数据。更新从表数据，主表数据不变

3、SET NULL 

删除：删除主表数据时自动更新从表对于数据值为NULL。删除从表数据，主表数据不变 

更新：更新主表数据时自动更新从表数据值为NULL。更新从表数据数据，主表不变

**关联**

1、UNION:

UNION 语句：用于将不同表中相同列中查询的数据展示出来，不包括重复数据；

UNION ALL 语句：用于将不同表中相同列中查询的数据展示出来，包括重复数据。

UNION ALL效率高

2、JOIN：

JOIN用来来联合多表查询。JOIN按照功能大致分为如下三类：

INNER JOIN（内连接或等值连接）：获取两个表中字段匹配关系的记录，即交集。

LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录。

RIGHT JOIN（右连接）：获取右表所有记录，即使左表没有对应匹配的记录。

## 逻辑架构

如下图所示，MySQL服务器逻辑架构从上往下可以分为三层：

第一层：处理客户端连接、授权认证等。

第二层：服务器层，负责sql语句的解析、优化、缓存以及内置函数的实现、存储过程等。

第三层：存储引擎，负责MySQL中数据的存储和提取。

### SQL查询流程

一条 SQL查询的过程，在mysql服务端大致就是这样的流程：**连接池、查询缓存（配置中开启了）、解析器、优化器、执行器。**


**连接池**

连接池负责客户端的连接管理、授权认证。建立连接后，会产生相应的连接信息，可以通过 show processlist 命令查看所有的连接信息。建立的连接基于Tcp建立，该命令可以看到连接的客户端的socket套接字ip+port。

**查询缓存**

在获取一个查询请求后，MySQL会先到查询缓存进行查看，如果select语句在查询缓存中能够找到，则直接返回结果给客户端，跳过解析、优化、执行阶段。如果select语句没能在查询缓存中找到，则继续后面的解析、优化、执行阶段。但是，查询缓存非常容易失效。因为只要一个表有更新操作，那这个表所有的查询缓存都会被清空。对一个承载正常业务的数据库来说，更新操作是非常频繁的，这就意味着查询缓存经常失效，从而导致查询缓存的命中率非常低。所以，使用查询缓存反而会给数据库带来额外的负担，在实际生产环境中，建议关闭查询缓存。

**解析器**

如果没有命中查询缓存，接下来就要进入解析器阶段了。解析器负责sql的词法解析和语法解析。

**优化器**

从解析器出来，就到了优化器阶段。优化器负责找到最优的执行计划，也就是决定SQL语句的执行方案。一条查询可以有很多种执行方式，最后都返回相同的结果 。比如下面这个查询SQL，查询表a中字段id等于1的值。可以遍历表a所有行，找出所有id等于1的值；也可以通过索引idx_id，找到id等于1的值。当然，前提是字段id有创建索引idx_id。两种方案的结果是一样的，但是执行效率不一样，优化器的作用就是选择最优的执行方案。

```
select id from a where id=1;
```

**执行器**

执行器负责调用存储引擎的接口，拿到查询结果。

### SQL更新流程

```
update a set value=value+1 where id=1;
```

1. 执行器通过存储引擎找到id=1这一行。如果id=1这一行的数据页在内存中，直接返回给执行器；如果不在，则要从磁盘读入内存，再返回给执行器；
2. 执行器拿到id=1这一行数据，把value值加1，产生一行新的数据，调用引擎接口写入这行新的数据；
3. 引擎将新的这行数据更新到内存中，同时将这个操作记录到Innodb的redo log日志中，此时redo log处于prepare状态；
4. 执行器生成更新操作的binlog，并写入磁盘；
5. 执行器调用引擎的事务提交接口，引擎将刚刚写入的redo log改成commit提交状态，更新完成。

## 物理组成

### 分类

MySQL大致上可以分为**日志文件、数据文件、配置文件等**，如下。

- 日志文件包括**二进制binlog日志、InnoDB redo 日志、InnoDB undo日志**、错误日志、慢查询日志、一般查询日志等。
- 数据文件主要指不同存储引擎的物理文件，不同数据文件的扩展名是不一样的，如 InnoDB 用 .ibd、MyISAM 用 .MYD。
- 除日志文件和数据文件外，还有配置文件my.cnf、pid 文件 mysql.pid、socket 文件 mysql.sock。


1. 二进制日志：二进制日志指的是**binlog日志文件，以二进制形式，将所有修改数据的记录到日志文件中。**
2. InnoDB redo log：是存储引擎 InnoDB 生成的日志，主要为了保证数据的可靠性。redo log 记录了 InnoDB 所做的所有物理变更和事务信息。
3. 错误日志：记录 MySQL 每次启动关闭的详细信息，以及运行过程中比较严重的警告和错误信息。
4. 慢查询日志：记录 MySQL 中执行时间较长的 query，包括执行时间、执行时长、执行用户、主机等信息。可配置。
5. 一般查询日志：记录 MySQL 中所有的 query。可配置。
6. .frm文件：.frm 文件存放表相关的元数据，包括表结构信息等。每张表都有一个对应的 .frm 文件（8.0以后表的元数据都放在了系统表空间了）。
7. .ibd 文件和 ibdata 文件： .ibd 文件（独享表空间的存储方式）和 ibdata 文件（共享表空间的存储方式）都是 InnoDB 引擎的数据文件（存储数据和索引），是否开启独享表空间可配置。

```bash
sh-4.2# ls /var/lib/mysql/RUNOOB/
Websites.ibd  employee_tbl.ibd  person_tb1.ibd  tcount_tbl.ibd    user.ibd
apps.ibd      orders.ibd        runoob_tbl.ibd  test_alt_tb1.ibd  user_history.ibd
```

### 日志

> 参考：https://zhuanlan.zhihu.com/p/33504555   https://juejin.cn/post/6860252224930070536

binlog二进制日志、undo日志、redo日志、错误日志、慢查询日志、、、

**binlog日志**

binlog是Mysql sever层维护的一种二进制日志，与innodb引擎中的redo/undo log是完全不同的日志；其主要是用来记录对mysql数据更新或潜在发生更新的SQL语句。binlog是mysql的逻辑日志（逻辑日志可以简单理解为记录的就是sql语句；物理日志记录的就是数据页变更），并且由Server层进行记录，使用任何存储引擎的mysql数据库都会记录binlog日志。binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。binlog作用如下：

- 复制：MySQL Replication在Master端开启binlog，Master把它的二进制日志传递给slaves并回放来达到master-slave数据一致的目的，实现主从复制
- 数据恢复：通过mysqlbinlog工具恢复数据。 恢复，就是让mysql将保存在binlog日志中指定段落区间的sql语句逐个重新执行一次。

默认情况下binlog日志是二进制格式，查看binlog方式：

```bash
show binary logs;
show master status; --query bin log is writing
show binlog events in 'binlog.000002' LIMIT 10;
## mysqlbinlog工具，显示更友好
mysqlbinlog --no-defaults  /var/lib/mysql/binlog.000002 --start-position=6785401
```

binlog刷盘时机：

对于InnoDB存储引擎而言，只有在事务提交时才会记录biglog，此时记录还在内存中，那么biglog是什么时候刷到磁盘中的呢？mysql通过**sync_binlog参数控制biglog的刷盘时机**，取值范围是0-N：

- 0：不去强制要求，由系统自行判断何时写入磁盘；
- 1：每次commit的时候都要将binlog写入磁盘；
- N：每N个事务，才会将binlog写入磁盘。

sync_binlog最安全的是设置是1，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。

binlog日志格式：

binlog日志有三种格式，分别为STATMENT、ROW和MIXED。在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式**通过binlog-format指定**。

- STATMENT 基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、slepp()等。
- ROW 基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题； 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨。
- MIXED 基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog。

**redo log**

mysql是如何保证持久性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：
1、因为Innodb是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了；
2、一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差。
因此设计了**redo log，具体来说就是只记录事务对数据页做了哪些修改**，这样就能完美地解决性能问题了，相对而言**redo log文件更小并且是顺序IO**。在innodb中，既有redo log需要刷盘，还有数据页也需要刷盘，redo log存在的意义主要就是降低对数据页刷盘的要求。

redo log实现包括两部分：一个是内存中的日志缓冲(redo log buffer)，另一个是磁盘上的日志文件(redo log file)。**mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个操作记录写到redo log file**。这种先写日志，再写磁盘的技术就是MySQL里经常说到的WAL(Write-Ahead Logging) 技术。在计算机操作系统中，用户空间(user space)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间(kernel space)缓冲区(OS Buffer)。因此，redo log buffer写入redo log file实际上是先将数据写入OS Buffer，然后再通过系统调用fsync()将其刷到redo log file中。如下图

![img](https://img-blog.csdnimg.cn/img_convert/1b08b33f7ce5c6c23d5712d4ae98c502.png)

mysql支持三种将redo log buffer写入redo log file的时机，可以通过**innodb_flush_log_at_trx_commit参数**配置，各参数值含义如下：

- 0（延迟写）事务提交时不会将redo log buffer中日志写入到os buffer，而是每秒写入os buffer并调用fsync()写入到redo log file中。也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。
- 1（实时写，实时刷）事务每次提交都会将redo log buffer中的日志写入os buffer并调用fsync()刷到redo log file中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。
- 2（实时写，延迟刷）每次事务提交都仅写入到os buffer，然后是每秒调用fsync()将os buffer中的日志写入到redo log file。

![img](https://img-blog.csdnimg.cn/img_convert/6a8648b2e1d69a4d1a40f86274c12d4a.png)

redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。


**binlog和redo log的区别：**

- 作用不同：redo log是用于crash recovery宕机恢复的，保证MySQL宕机也不会影响持久性；binlog是用于主从复制和数据的备份恢复的。
- 内容不同： binlog是逻辑日志，记录某个语句的基本逻辑，即SQL语句；redo log是物理日志，记录对某个数据页所做的修改；
- 层次不同：binlog是在MySQL的Server层实现，所有的存储引擎都可以使用binlog这个日志模块；redo log是InnoDB存储引擎特有的日志模块；
- 写入时机不同：binlog 是在事务最终提交前写入的；redo log是在事务执行过程不断的将DML语句写入缓冲区；
- 写入方式不同：binlog是追加写，在写满或重启之后，会生成新的binlog文件，之前的日志不会进行覆盖；redo log是循环写，空间大小是固定的；

**undo log：**

- undo回退日志，用于保证事务的**原子性、隔离性**的基础MVCC中会用到

## 存储引擎

### MyISAM

MyISAM使用的场景比较少，主要特点：

- 不支持事务操作，ACID 的特性也就不存在了。
- 不支持外键操作，如果强行增加外键，MySQL 不会报错，只不过外键不起作用。
- MyISAM 默认的**锁粒度是表级锁，所以并发性能比较差，加锁比较快，锁冲突比较少，不太容易发生死锁**的情况。
- MyISAM 会在磁盘上存储三个文件，文件名和表名相同，扩展名分别是 .frm(存储表定义)、.MYD(MYData,存储数据)、MYI(MyIndex,存储索引)。
- 数据库所在主机如果宕机，MyISAM 的数据文件容易损坏，而且难以恢复，即不支持crash-safe（指数据库发生故障重启，之前提交的数据不会丢失）。

### InnoDB

InnoDB 存储引擎最常用，主要特点：

- 支持事务操作，具有事务 ACID 隔离特性，默认的隔离级别是可重复读(repetable-read)、通过MVCC（并发版本控制，后续研究。。。）来实现的，能够解决脏读和不可重复读的问题。
- InnoDB 支持外键操作。
- InnoDB **默认的锁粒度行级**锁，并发性能比较好，但可能会发生死锁的情况。
- 和 MyISAM 一样的是，InnoDB 存储引擎也有 .frm文件存储表结构 定义，但是不同的是，InnoDB 的表数据与索引数据是存储在一起的。
- InnoDB 有安全的redo日志文件，这个日志文件用于恢复因数据库崩溃或其他情况导致的数据丢失问题，保证数据的一致性。

基本原则是能用InnoDB就用InnoDB，除非其他引擎提供了不可替代的功能。

## 其他

### 数据结构对比 

MySQL的数据类型分为三类：数值、日期/时间和字符串(字符)类型。

**varchar与char**

1、char：

- char表示定长字符串，长度是固定的，底层是数组；
- 如果插入数据的长度小于char的固定长度时，则用空格填充；
- 因为长度固定，所以存取速度要比varchar快很多，甚至能快50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法；
- 对于char来说，最多能存放的字符个数为255，和编码无关

2、varchar：

- varchar表示可变长字符串，长度是可变的，底层是链表；
- 插入的数据是多长，就按照多长来存储；
- varchar在存取方面与char相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法；
- 对于varchar来说，最多能存放的字符个数为65532。
- varchar(50)：最多存放50个字符，varchar(50)和(200)存储hello所占空间一样。

**int、TEXT和BLOB、浮点数和定点数**

- int(10)中10表示显示的数据的长度，不是存储数据的大小，仍占4字节存储，存储范围不变 只是交互的显示大小，无关计算和存储；
- BLOB 与TEXT： BLOB 能够保存二进制数据和大字符串数据；而 TEXT 只能保存大字符串数据。
- 浮点数和定点数：浮点数指的就是含有小数的值，浮点数插入到指定列中超过指定精度后，浮点数会四舍五入，MySQL 中的浮点数指的就是 float 和 double，定点数指的是 decimal，定点数能够更加精确的保存和显示数据。

### 内置的数据库表 

1. information_schema数据库又称为信息架构，数据表保存了MySQL服务器所有数据库的信息。如数据库名，数据库的表，表栏的数据类型与访问权限等。
2. performance_schema数据库主要用于收集数据库服务器性能参数，以便优化mysql数据库性能。
3. mysql数据库是存储着已MySQL运行相关的基本信息等数据管理的数据库。
4. sys 数据库是mysql5.7增加的，通过这个库可以快速的了解系统的元数据信息，这个库可以方便DBA发现数据库的很多信息，提供解决性能瓶颈的信息。

# 事务

MySQL中默认采用的是自动提交autocommit模式。在自动提交模式下，如果没有start transaction显式地开始一个事务，那么每个sql语句都会被当做一个事务执行提交操作。事务必须满足ACID：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。

- 原子性：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
- 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE。
- 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。
- 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。

**事务的应用场景**

1. 在人员管理系统中，删除一个人员，你既需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等；
2. 银行系统的转账。

**应用命令**

```
BEGIN：显式地开启一个事务；
COMMIT：提交事务，并使已对数据库进行的所有修改成为永久性的；
ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改；
SAVEPOINT identifier：SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT；
RELEASE SAVEPOINT identifier：删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常；
ROLLBACK TO identifier：把事务回滚到标记点；
SET TRANSACTION 用来设置事务的隔离级别。
```

## 事务的隔离水平

**并发冲突问题**：脏读、不可重复读、幻读、更新丢失

读-写的冲突：

1. 读未提交，也称脏读，脏读发生在一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。
2. 不可重复读：不可重复读发生在**一个事务中执行相同的查询两次或两次以上，但是每次都得到不同的数据时**。这通常是因为另一个并发事务在两次查询期间进行了更新。不可重复读重点是修改数据导致的(修改数据时排他读);
3. 幻读：幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。幻读重点是插入或者删除数据导致的（对满足条件的数据行集进行锁定）。

写-写的冲突：

1. 更新丢失，包括第一类更新丢失，第二类更新丢失。

防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此**，防止更新丢失应该是应用层的责任。**

**脏读、不可重复读、幻读都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决**：一种是加锁：在读取数据前，对其加锁，阻止其他事务对数据进行修改。另一种是数据多版本并发控制MVCC：不用加任何锁， 通过一定机制生成一个数据请求时间点的一致性数据快照 （Snapshot)。

**隔离水平**

隔离水平越高，数据的完整性也就越高，但同时运行性下降，相反如果隔离水平越低数据完整性越低，同时运行性也就提高了。四种隔离水平：

1. READ_UNCOMMITTED：这是事务最低的分离水平，它充许别外一个事务可以看到这个事务未提交的数据，会出现脏读、不可重复读、幻读 （分离水平最低，并发性能高）
2. READ_COMMITTED：保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。可以避免脏读，但会出现不可重复读、幻读问题（锁定正在读取的行）
3. REPEATABLE_READ：默认的隔离级别。可以防止脏读、不可重复读，但会出幻读（锁定所读取的所有行），mysql默认的隔离级别
4. SERIALIZABLE：这是花费最高代价但是最可靠的事务分离水平，事务被处理为顺序执行。保证所有的情况不会发生（锁表,并发性及其低）

## ACID原理

### 原子性

原子性是指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做；如果事务中一个sql语句执行失败，则已执行的语句也必须回滚，数据库退回到事务前的状态。原子性通过undo log回滚日志实现。

**undo log**

实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。

undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。

### 持久性

持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。持久性通过redo log日志实现。

**redo log**

redo log和undo log都属于InnoDB的事务日志。InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘IO，效率会很低。为此，InnoDB提供了**缓冲池**(Buffer Pool)**，**Buffer Pool中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，**Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。**Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。

redo log被引入来解决这个问题：当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync强制同步接口对redo log进行刷盘（其中之一的策略）。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。

问题来了，既然redo log也需要在事务提交时将日志写入磁盘，为什么它比直接将Buffer Pool中修改的数据写入磁盘(即刷脏)要快呢？主要有以下两方面的原因：

1. 刷脏是随机IO，因为每次修改的数据位置随机，但写**redo log是追加操作，属于顺序IO。**
2. 刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而redo log中只包含真正需要修改写入的部分，无效IO大大减少。

前面曾提到：当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略，修改innodb_flush_log_at_trx_commit参数可以改变该策略，但事务的持久性将无法保证。除了事务提交时，还有其他对redo log进行刷盘时机：如其他的策略master thread每秒刷盘一次redo log等，这样的好处是不一定要等到事务commit提交时刷盘，commit速度大大加快。

事务日志应用：

```bash
# 以下是undo+redo事务的简化过程，假设有2个数值，分别为A和B,值为1，2
1. start transaction;
2. 记录 A=1 到undo log;
3. update A = 3；
4. 记录 A=3 到redo log；
5. 记录 B=2 到undo log；
6. update B = 4；
7. 记录B = 4 到redo log；
8. 将redo log刷新到磁盘 ## 写文件先写到缓冲区，最后将缓冲区的数据刷盘到磁盘文件中
9. commit

#在1-8的任意一步系统宕机，事务未提交，该事务就不会对磁盘上的数据做任何影响。
#如果在8-9之间宕机，恢复之后可以选择回滚，也可以选择继续完成事务提交，因为此时redo log已经持久化。若在9之后系统宕机，内存映射中变更的数据还来不及刷回磁盘，那么系统恢复之后，可以根据redo log把数据刷回磁盘。
#redo log其实保障的是事务的持久性和一致性，而undo log则保障了事务的原子性。
```

### 隔离性

与原子性、持久性侧重于研究事务本身不同，隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。严格的隔离性，对应了事务隔离级别中的Serializable (可串行化)，但实际应用中出于性能方面的考虑很少会使用可串行化。

隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们主要考虑最简单的读操作和写操作(加锁读等特殊读操作会特殊说明)，那么隔离性的探讨，主要可以分为两个方面：**加锁机制保证隔离性、MVCC保证隔离性**。

其中MVCC（Multi-Version Concurrency Control），即多版本的并发控制协议。**MVCC最大的优点是读不加锁，因此读写不冲突，并发性能好**。InnoDB实现MVCC，多个版本的数据可以共存，MVCC主要介绍见下文。

**加锁读与next-key lock** 

按照是否加锁，MySQL的读可以分为两种：一种是非加锁读，也称作一致性快照读，使用普通的默认的select语句，这种情况下使用MVCC机制避免了脏读、不可重复读、幻读，保证了隔离性。另一种是加锁读，查询语句有所不同，如下所示：

```sql
--共享锁读取
SELECT * FROM user  WHERE id='1' lock in share mode;
--排它锁读取
SELECT * FROM user  WHERE id='1' for update;
--MVCC
SELECT * FROM user  WHERE id='1';
```

**加锁读解决脏读、不可重复读、幻读：**

加锁读在查询时会对查询的数据加锁（共享锁或排它锁）。由于锁的特性，当某事务对数据进行加锁读后，其他事务无法对数据进行写操作，因此可以避免脏读和不可重复读。而**避免幻读，则需要通过next-key lock。next-key lock是行锁的一种，实现相当于record lock(记录锁) + gap lock(间隙锁)**；其特点是不仅会锁住记录本身(record lock的功能)，还会锁定一个范围(gap lock的功能)。因此，加锁读同样可以避免脏读、不可重复读和幻读，保证隔离性。

InnoDB实现的RR可重复读，通过锁机制（包含next-key lock）、MVCC（包括数据的隐藏列、基于undo log的版本链、ReadView）等，实现了一定程度的隔离性，可以满足大多数场景的需要。不过需要说明的是，RR虽然避免了幻读问题，但是毕竟不是Serializable，不能保证完全的隔离。

## MVCC

> https://juejin.cn/post/6871046354018238472 
> https://cloud.tencent.com/developer/article/1488871

MVCC全称Multi-Version Concurrency Control，即多版本并发控制，主要是为了提高数据库的并发性能。MVCC是“维持一个数据的多个版本，使读写操作没有冲突”的一个抽象概念，通过快照读实现。MVCC指的就是**在使用READ COMMITTD、REPEATABLE READ这两种隔离级别的事务在执行普通的SEELCT操作时访问记录的版本链的过程**，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。

### 当前读和快照读

**当前读**

它读取的数据库记录，都是当前最新的版本，会对当前读取的数据进行加锁，防止其他事务修改数据，是悲观锁的一种操作。如下操作都是当前读：

- select lock in share mode (共享锁)
- select for update (排他锁)
- update、insert、delete (排他锁)
- 串行化事务隔离级别

**快照读**

快照读的实现是基于多版本并发控制，即MVCC，既然是多版本，快照读读到的数据不一定是当前最新的数据，有可能是之前历史版本的数据。如下操作是快照读：

- 不加锁的select操作（当事务级别不是串行化）

加锁和mvcc如何解决幻读问题：

快照读：通过MVCC来进行控制的，不用加锁。按照MVCC中规定的“语法”进行增删改查等操作，可以避免幻读。

当前读：通过next-key锁（行锁+gap锁 锁住一定的范围）来解决问题的。

### MVCC解决的问题

mvcc用来解决读—写冲突的无锁并发控制，就是为事务分配单向增长的时间戳。为每个数据修改保存一个版本，版本与事务时间戳相关联。读操作只读取该事务开始前的数据库快照。解决问题如下：

- 并发读-写时：可以做到读操作不阻塞写操作，同时写操作也不会阻塞读操作。
- 解决脏读、幻读、不可重复读等事务隔离问题，但不能解决上面的写-写更新丢失问题。因此有了下面提高并发性能的组合拳：

1. MVCC + 悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突
2. MVCC + 乐观锁：MVCC解决读写冲突，乐观锁解决写写冲突

### MVCC原理

MVCC的实现原理主要是**事务id、版本链，undo日志 ，Read View 读视图**来实现的

#### 版本链

我们数据库中的每行数据，除了我们肉眼看见的数据，还有几个隐藏字段。分别是db_trx_id、db_roll_pointer、db_row_id。

- db_trx_id：6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID。
- db_roll_pointer（版本链关键）：7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里）
- db_row_id：6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以db_row_id产生一个聚簇索引。
- 实际还有一个删除flag隐藏字段, 记录被更新或删除并不代表真的删除，而是删除flag变了

![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d2fa1c7ffdf241a09a790a34b4b8c817~tplv-k3u1fbpfcp-zoom-1.image) 

如上图，db_row_id是数据库默认为该行记录生成的唯一隐式主键，db_trx_id是当前操作该记录的事务ID，而**db_roll_pointer是一个回滚指针，用于配合undo日志，指向上一个旧版本**。

每次对数据库记录进行改动，都会记录一条undo日志，每条undo日志也都有一个roll_pointer属性（INSERT操作对应的undo日志没有该属性，因为该记录并没有更早的版本），可以将这些undo日志都连起来，串成一个链表，所以现在的情况就像下图一样：

对该记录每次更新后，都会将旧值放到一条undo日志中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被roll_pointer属性连接成一个链表，我们把这个链表称之为版本链，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的事务id，这个信息很重要，在**根据ReadView判断版本可见性**的时候会用到。

#### undo日志

Undo log 主要用于记录数据被修改之前的日志，在表信息修改之前先会把数据拷贝到undo log里。当事务进行回滚时可以通过undo log 里的日志进行数据还原。

Undo log 的用途：

1. 保证事务进行rollback时的原子性和一致性，当事务进行回滚的时候可以用undo log的数据进行恢复。
2. 用于MVCC快照读的数据，在MVCC多版本控制中，通过读取undo log的历史版本数据可以实现不同事务版本号都拥有自己独立的快照数据版本。

undo log主要分为两种：

1. insert undo log。代表事务在insert新记录时产生的undo log , 只在事务回滚时需要，并且在事务提交后可以被立即丢弃
2. update undo log（主要）。事务在进行update或delete时产生的undo log ，不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被**purge**线程统一清除

Purge操作控制undo日志的回收和真正删除已标记删除的记录。undo中保存老记录的历史版本, 当这些历史版本不再需要时，交由purge清理。undo日志应该及时purge，undo日志的堆积不仅会导致回滚段空间的增长，而且delete mark的记录没有真正删除，也会影响查询的效率。

undo日志清理策略详见：https://cloud.tencent.com/developer/article/1488871

#### Read View

事务进行快照读操作select的时候产生读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照。记录并维护系统当前**活跃事务的ID(没有commit，当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以越新的事务，ID值越大)**，是系统中当前不应该被本事务看到的其他事务id列表。

**Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。**

Read View几个属性：

- trx_ids: 当前系统活跃(未提交)事务版本号集合。
- low_limit_id: 创建当前read view 时“当前系统最大事务版本号+1”。
- up_limit_id: 创建当前read view 时“系统正处于活跃事务最小版本号”
- creator_trx_id: 创建当前read view的事务版本号；

**Read View可见性判断条件：**

1、db_trx_id < up_limit_id || db_trx_id == creator_trx_id（显示）

如果数据事务ID小于read view中的最小活跃事务ID，则可以肯定该数据是在当前事务开启之前就已经存在了的,所以可以显示。

或者数据的事务ID等于creator_trx_id ，那么说明这个数据就是当前事务自己生成的，自己生成的数据自己当然能看见，所以这种情况下此数据也是可以显示的。

2、db_trx_id >= low_limit_id（不显示）

如果数据事务ID大于read view 中的当前系统的最大事务ID，则说明该数据是在当前read view 创建之后才产生的，所以数据不显示。如果小于则进入下一个判断

3、db_trx_id是否在活跃事务（trx_ids）中

不存在：则说明read view产生的时候事务已经commit了，这种情况数据则可以显示。

已存在：则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的。

> ReadView应用参考：https://zhuanlan.zhihu.com/p/133480167

这些记录都是去版本链里面找的，先找最近记录，如果最近这一条记录事务id不符合条件，不可见的话，再去找上一个版本再比较当前事务的id和这个版本事务id看能不能访问，以此类推直到返回可见的版本或者结束。

**已提交读和可重复读下快照读的区别**

上面所讲的Read View用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

在ReadCommit隔离级别下，事务中的每个语句执行前都会分配一次ReadView。 在RepeatableRead隔离级别下，只在事务开始时才分配一次ReadView。

在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View， 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见；

即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见

而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因。

## 锁

### 行锁、表锁和页锁

1、行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。

特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

InnoDB是基于索引来完成行锁。

```sql
select * from tab_with_index where id = 1 for update;
```

for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁。

InnoDB存储引擎的行锁的三种算法如下，具体原理待研究。。。

- Record lock：单个行记录上的锁
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- Next-key lock：record+gap 锁定一个范围，包含记录本身

2、表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。

特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。

3、页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，**一次锁定相邻的一组记录。**

特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

不同的引擎默认不同锁的粒度，数据库锁分为**行级锁(INNODB引擎默认)**、表级锁(MYISAM引擎默认)和页级锁(BDB引擎默认 )。

### 共享锁、排他锁

共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。

排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。

锁的粒度取决于具体的存储引擎，InnoDB实现了行级锁，页级锁，表级锁。他们的加锁开销从大到小，并发能力也是从大到小。

### 悲观锁、乐观锁

悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制

乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：使用版本号机制MVCC或CAS自旋算法实现。

两种锁的使用场景：

乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。

但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行重试，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。

### 死锁

指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。

常见的解决死锁的方法：

1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。

2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；

3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；

4、如果业务处理不好可以用分布式事务锁或者使用乐观锁

# 索引

> 参考：https://cloud.tencent.com/developer/article/1691812

索引优点：索引可以大大提高查询检索速度。**数据即索引，索引即数据**。通过快速查找到索引可以查找到存储数据的数据页的地址。

索引缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。建立索引会占用磁盘空间的索引文件。

索引的数据结构：哈希表、B树、B+树

## 分类

按照用途分为：

普通索引INDEX、主键索引PRIMARY KEY（唯一且不允许为空）、唯一性索引UNIQUE（索引列的值唯一但允许为空）、全文索引FULLTEXT INDEX（搜索引擎使用的一种关键技术）。

按照作用列个数分为：

- 单列索引，只作用于一列；
- 联合索引，即一个索引包含多个列。联合索引的最左前缀原则：联合索引是按照索引列的顺序，从第一列开始进行排序的。如果查询条件跳过了第一列，那么其实是无序的，就无法走索引，只能全表扫描。

按照数据结构分类：

- 哈希索引：哈希索引是 MySQL 中用到的唯一 key-value 键值对的数据结构，很适合作为索引。HASH 索引具有一次定位的好处，不需要像树那样逐个节点查找，但是这种查找适合应用于查找单个键的情况，对于范围查找，HASH 索引的性能就会很低。默认情况下，MEMORY 存储引擎使用 HASH 索引，但也支持 BTREE 索引。hash索引底层就是hash表，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际的数据。
- B-Tree 索引：BTree 是一种平衡树，它有很多变种，最常见的就是 B+ Tree，它被 MySQL 广泛使用。B+树底层实现是多路平衡查找树。对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。
- R-Tree 索引：R-Tree 在 MySQL 很少使用，仅支持 geometry 数据类型，支持该类型的存储引擎只有MyISAM、BDb、InnoDb、NDb、Archive几种，相对于 B-Tree 来说，R-Tree 的优势在于范围查找
- 全局索引：全局索引FULLTEXT，目前只有 MyISAM 引擎支持全局索引，它的出现是为了解决针对文本的模糊查询效率较低的问题，并且只限于 CHAR、VARCHAR 和 TEXT 列。

## 最左匹配原则

https://cloud.tencent.com/developer/article/1449114
https://zhuanlan.zhihu.com/p/161002071

联合索引的最左匹配原则。对多个字段同时建立的索引。**联合索引是有顺序的**，ABC，ACB是完全不同的两种联合索引。以联合索引(a,b,c)为例，建立这样的索引相当于建立了索引a、ab、abc三个索引。一个索引顶三个索引当然是好事，但是每多一个索引都会增加写操作的开销和磁盘空间的开销，需要谨慎使用。(A,B,C) 这样3列，mysql会首先匹配A，然后再B，C。如果用(B,C)这样的数据来检索的话，就会找不到A使得索引失效。**建立联合索引的原则是把最常用的，筛选数据最多的字段放在左侧。**当执行以下查询的时候，是无法使用这个联合索引的。因为联合索引中是先根据A进行排序的。如果A没有先确定，直接对B和C进行查询的话，就相当于乱序查询一样，因此索引无法生效，查询是全表查询。

```sql
select * from STUDENT where B='b';
```

联合索引：

![1620140263100](C:\Users\ADMINI~1\AppData\Local\Temp\1620140263100.png)

应用：

![1620137858004](C:\Users\ADMINI~1\AppData\Local\Temp\1620137858004.png)

type: all<index<ref

index：这种类型表示是 MySQL 会对整个该索引进行扫描。要想用到这种类型的索引，对这个索引并无特别要求，只要是索引，或者某个复合索引的一部分，MySQL 都可能会采用  Index  类型的方式扫描。但是呢，缺点是效率不高，MySQL 会从索引中的第一个数据一个个的查找到最后一个数据，直到找到符合判断条件的某个索引。 和全表扫描效率差不多。

ref：这种类型表示 MySQL 会根据特定的算法快速查找到某个符合条件的索引，而不是会对索引中每一个数据都进行一一的扫描判断，也就是所谓你平常理解的使用索引查询会更快的取出数据。而要想实现这种查找，索引却是有要求的，要实现这种能快速查找的算法，索引就要满足特定的数据结构。简单说，也就是索引字段的数据必须是有序的，才能实现这种类型的查找，才能利用到索引。 

## B树和B+树

B+树与B树比较：B+树相较于B树最大的优势在于数据全部都存在于叶子节点，叶子节点间以指针相互连接，这样在进行按照索引的范围查找的时候就只需要遍历前后指针就可以完成，而B树要一个一个索引去进行查找，效率差别很大。

B+树与hash比较：B+树相较于hash的优势在于B+树不用一次将数据全部加载到内存，而是先确定要查询索引的地址，将对应的地址的索引加载到内存。而hash需要将全部的数据一次性加载到内存才能完成查找。

**B树**

1. 树的每个节点，存储多个索引元素，同时存储索引对应的数据。
2. 叶节点具有相同的深度，叶节点的指针为空。
3. 所有索引元素不重复。
4. 节点中的数据索引从左到右递增排列。

B-Tree中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个3阶的B-Tree。每个节点占用一个盘块的磁盘空间（每个节点的大小是固定的，一般为一页Page 16KB），一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。

![B-Tree](https://img-blog.csdnimg.cn/img_convert/2a06fc0e360228c85b657f7991cb6458.png)

模拟查找关键字29的过程：

```bash
1. 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】
   比较关键字29在区间（17, 35），找到磁盘块1的指针P2。
2. 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】
   比较关键字29在区间（26, 30），找到磁盘块3的指针P2。
3. 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】
   在磁盘块8中的关键字列表中找到关键字29。
```

分析上面过程，发现需要3次磁盘I/O操作，和3次内存查找操作。由于内存中的关键字是一个有序表结构，可以利用二分法查找提高效率。而3次磁盘I/O操作是影响整个B-Tree查找效率的决定因素。B-Tree相对于AVLTree缩减了节点个数，使每次磁盘I/O取到内存的数据都发挥了作用，从而提高了查询效率。

**B+树**

1. 非叶子节点不存储data，只存储索引(冗余)，可以放更多的索引。
2. 叶子节点包含所有索引字段和对应的数据。
3. 节点中的数据索引从左到右递增排列。
4. 叶子节点用**双向指针**连接，提高区间访问的性能。

B+的优势：树高度较矮，针对大多数的表，2~4层即可满足需求。区间访问性能较好。

B+Tree是在B-Tree基础上的一种优化，InnoDB存储引擎就是用B+Tree实现其索引结构。**在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。**由于B+Tree的非叶子节点只存储键值信息，假设每个磁盘块能存储4个键值及指针信息，则变成B+Tree后其结构如下图所示：

![B+Tree](https://img-blog.csdnimg.cn/img_convert/043c5b43f7217d0a0a5d54223a0739b2.png)

**B+树在查找、插入和删除等操作的时间复杂度均为logn  。** 

BST、二叉平衡树、红黑树。

B树和B+树区别：

- B+树的磁盘读写代价更低高度更低：非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度
- B+树的查询效率更稳定：任何的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当； 
- B+树便于范围查询：B+树只需要去遍历叶子节点就可以实现整棵树的遍历。 

## 聚集回表覆盖

**聚集索引：**

以innodb作为存储引擎的表，表中的数据都会有一个主键，即使你不创建主键，系统也会帮你创建一个隐式的主键。这是因为innodb是把数据存放在B+树中的，而B+树的键值就是主键，在B+树的叶子节点中，存储了表中所有的数据。**这种以主键作为B+树索引的键值而构建的B+树索引，我们称之为聚集索引**，聚集索引的表记录的排列顺序和索引的排列顺序一致。 对于主键索引，叶子节点的内容是所在行的所有数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。 

**非聚集索引：**

以主键以外的列值作为键值构建的B+树索引，我们称之为非聚集索引。非聚集索引与聚集索引的区别在于非聚集索引的叶子节点不存储表中的数据，而是存储该列对应的主键，**想要查找数据我们还需要根据主键再去聚集索引中进行查找，这个再根据聚集索引查找数据的过程，我们称为回表**。

对于非主键索引，叶子节点的内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。基于非主键索引的查询时，需要根据查询到的主键值，再去主键索引查询一次记录，这个过程称为**回表**。回表会导致多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

**回表：**

https://zhuanlan.zhihu.com/p/107125866

先通过普通索引的值定位聚簇索引值，再通过聚簇索引的值定位行记录数据，需要扫描两次索引B+树，它的性能较扫一遍索引树更低。 

**索引覆盖：**

从非聚集索引中就可以得到要查询的记录，而不需要回表再去查询聚集索引中的记录。只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表，速度更快。 

## 注意点

> 参考：https://zhuanlan.zhihu.com/p/88963084 https://blog.csdn.net/u012758088/article/details/77140686

**建立索引原则**

1. 选择索引位置，选择索引最合适的位置是出现在 where 语句中的列，而不是 select 关键字后的选择列表中的列。
2. 选择使用唯一索引，顾名思义，唯一索引的值是唯一的，可以更快速的确定某条记录，例如学生的学号就适合使用唯一性索引，而学生的性别则不适合使用，因为不管搜索哪个值，都差不多有一半的行。
3. 为经常使用作为查询条件的字段建立索引，可以提高整个表的查询速度。
4. 为经常需要排序、分组和联合操作的字段建立索引，经常需要ORDER BY、GROUP BY、DISTINCT和UNION等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。
5. 不要过度索引，限制索引数目，索引的数目不是越多越好，每个索引都会占据磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。
6. 尽量使用前缀索引，如果索引的值很长，那么查询速度会受到影响，这个时候应该使用前缀索引（该字段的前几位就尽可能唯一确定一条记录行），对列的某几个字符能确定唯一一行记录时可以作为索引，提高检索效率。
7. 利用最左前缀，在创建一个 n 列的索引时，实际上是创建了 MySQL 可利用的 n 个索引。多列索引可以起到几个索引的作用，利用索引最左边的列来匹配行，这样的列称为最左前缀。比如a = 1 and b = 2 and c > 3 and d = 4，如果建立（a,b,c,d）顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
8. =和in可以乱序。比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。
9. 尽量选择区分度高的列作为索引。区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就 是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条 记录。
10. 索引列不能参与计算，保持列干净。比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本 太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);
11. 尽量的扩展索引，不要新建索引。 比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。
12. 当单个索引字段查询数据很多，区分度都不是很大时，则需要考虑建立联合索引来提高查询效率。
13. 对于使用 InnoDB 存储引擎的表来说，记录会按照一定的顺序保存。如果有明确的主键定义，那么会按照主键的顺序进行保存；如果没有主键，但是有唯一索引，那么就按照唯一索引的顺序进行保存。如果既没有主键又没有唯一索引，那么表中会自动生成一个内部列，按照这个隐藏的列的顺序进行保存。一般来说，使用主键的顺序是最快的
14. 删除不再使用或者很少使用的索引

**避免全表扫描-索引失效**

1. 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0
2. 应尽量避免在 where 子句中使用!=或<>操作符，否则引擎将放弃使用索引而进行全表扫描。
3. 应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num=10 or num=20可以这样查询：select id from t where num=10 union all select id from t where num=20
4. in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3
5. 避免在索引列上使用计算，也就是说，应尽量避免在 where 子句中对字段进行表达式操作和函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num/2=100应改为:select id from t where num=100*2   select id from t where substring(name,1,3)='abc' ，name以abc开头的id，应改为:select id from t where name like 'abc%'
6. 用 exists 代替 in 是一个好的选择：exists用于检查子查询是否至少会返回一行数据，该子查询实际上并不返回任何数据，而是返回值true或false。select num from a where num in(select num from b) 用下面的语句替换：select num from a where exists (select 1 from b where num=a.num)
7. 任何地方都不要使用 select * from t ，用具体的字段列表代替，不要返回用不到的任何字段。
8. 用>=替代> 高效: SELECT * FROM EMP WHERE DEPTNO >=4 低效: SELECT * FROM EMP WHERE DEPTNO >3

## 执行计划

详见： https://cloud.tencent.com/developer/article/1666118

explain + 查询SQL - 用于显示SQL执行信息参数，根据参考信息可以进行SQL优化。索引的命中查看。

## 其他

问题

1.  为什么 InnoDB 表必须有主键，且推荐整型的自增主键？InnoDB 的表数据文件(.ibd)，就是按照B+树结构，根据主键索引组织起来的一个索引结构文件，因此一定要有一个主键。如果用户没有自定义主键，InnoDB会选择一列唯一索引作为主键。如果没有唯一索引，InnoDB 会为每行数据生成一个唯一的整型自增数值rowId(隐藏列)，作为主键来组织整个索引文件。**使用整型主键，索引查询时，比较效率较高。且整型字段所占空间较小。 使用自增主键，大部分的插入操作，都是在叶子节点链表上的addLast，不会涉及到节点的页分裂和整棵树的平衡操作，插入效率很高。**
2.  为什么 **InnoDB 的的非主键索引，存储的是主键索引的值**，而不是像主键索引一样直接存储数据？ 
    1. **数据一致性角度：如果数据在多个索引处维护，那么就存在数据一致性问题。插入一条记录时，需要在每个索引树上都插入一遍，就涉及到了分布式事务的问题。**
    2. **存储空间角度：如果所有索引树都保存数据，会造成大量的空间浪费。**
3.  在 InnoDB 引擎下，重建主键索引，无论是新增还是删除，都会导致整张表进行重建。可以使用 alter table T engine=InnoDB 来重建主键索引。

# 内置特性

## 视图

视图view是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。作为一个select语句保存在数据字典中的。通过视图，可以展现基表的部分数据；视图数据来自定义视图的查询中使用的表，使用视图动态生成。

```sql
CREATE VIEW user_v AS SELECT * FROM user;
SELECT * FROM user_v;
CREATE VIEW user_order_v AS SELECT u.username, o.createtime FROM user u JOIN orders o ON u.id = o.user_id;
SELECT * FROM user_order_v;
+----------+---------------------+
| username | createtime          |
+----------+---------------------+
| 123      | 2020-10-30 00:00:00 |
+----------+---------------------+
1 row in set (0.00 sec)
```

优点：

- 简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。
- 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。
- 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。
- 总而言之，使用视图的大部分情况是为了保障数据安全性，提高查询效率。

缺点：

- 且操作视图的很多命令都与普通表一样，这会导致在业务代码中无法通过sql区分表和视图，使代码变得复杂。
- 会给数据库带来额外的开销，不要给数据库过大的压力。

## 存储过程

存储过程就是数据库中保存的一系列SQL命令的集合，类似一个sql函数，支持分支、循环、简单异常处理等语法。sql中的变量类型：局部变量、用户变量@、全局变量(对所有的客户端有效)、会话变量(只对连接的客户端有效)。

```sql
--in param && if else
DELIMITER //
CREATE PROCEDURE sp_search_user (IN name VARCHAR(32))
BEGIN
IF name IS NULL OR name = '' THEN
    SELECT * FROM user;
ELSE
    SELECT * FROM user WHERE username LIKE name;
END IF;
END
//
DELIMITER ;
CALL sp_search_user('陈%');
--out param
DROP PROCEDURE IF EXISTS sp_max_num;
DELIMITER //
CREATE  PROCEDURE  sp_max_num (OUT max_num INT)
BEGIN
SELECT max(id) INTO max_num FROM user;
END;
//
DELIMITER ;
CALL sp_max_num(@max_num);
SELECT @max_num;
--loop statement
DROP PROCEDURE IF EXISTS sp_loop;
DELIMITER //
CREATE  PROCEDURE  sp_loop()
BEGIN
DECLARE n INT;
SET n=0;
WHILE n<20000 DO
INSERT INTO user (username, birthday, sex, address) VALUES (CONCAT('name', n), '1995-05-10', 1, '***');
SET n = n+1;
END WHILE;
END
//
DELIMITER ;
CALL sp_loop();
```

存储过程优点：

- 存储过程是预编译过的，执行效率高。
- 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通信的消耗。
- 安全性高，执行存储过程需要有一定权限的用户。
- 存储过程可以重复使用，减少数据库开发人员的工作量。

## 存储函数

两者类似，存储过程与存储函数的区别：

- 存储过程可以有多个in,out,inout参数，而存储函数只有输入参数类型
- 存储过程实现的功能要复杂一些；而存储函数的单一功能性(针对性)更强。
- 存储过程可以返回多个值；存储函数只能有一个返回值。
- 存储过程一般独立的来执行，通过call sp调用；而存储函数可以作为其他SQL语句的组成部分来出现，通过select fun调用。
- 存储过程可以调用存储函数。但存储函数不能调用存储过程。
- 存储过程性能非常高，一般用于批量执行语句。

```sql
-- 定义存储函数
DELIMITER $$
CREATE FUNCTION CustomerLevel(p_creditLimit double) RETURNS VARCHAR(10)
    DETERMINISTIC
BEGIN
    DECLARE lvl varchar(10);
    IF p_creditLimit > 50000 THEN
        SET lvl = 'PLATINUM';
    ELSEIF (p_creditLimit <= 50000 AND p_creditLimit >= 10000) THEN
        SET lvl = 'GOLD';
    ELSEIF p_creditLimit < 10000 THEN
        SET lvl = 'SILVER';
    END IF;
 RETURN (lvl);
END $$
DELIMITER ; 
-- 调用存储函数
SELECT customerName, CustomerLevel(creditLimit) FROM customers ORDER BY  customerName; 
```

## 触发器

触发器是用户定义在关系表上的一类由事件驱动的特殊的存储过程。触发器是指一段代码，当触发某个事件时，自动执行这些代码。触发器可以简单理解一种特殊的存储过程，之前存储过程的变量定义及流程语句同样适合触发器，唯一不同的是我们只需要定义触发器，而不用手动调用触发器。从事件触发的角度来说，触发器编写的过程就是触发事件定义的过程，因为触发器定义好后会随着数据库操作命令的执行而触发，这些具体的操作是INSERT/UPDATE/DELETE。适用场景：

- 可以通过数据库中的相关表实现级联更改，实现有外键的主从表的同步修改。
- 实时监控某张表中的某个字段的更改而需要做出相应的处理。
- 插入更新删除时留下数据的操作日志。

```sql
CREATE TABLE user_history SELECT * FROM user;
DELETE FROM user_history;
DELIMITER //
CREATE TRIGGER trg_user_history AFTER DELETE ON user FOR EACH ROW
BEGIN
INSERT INTO user_history (id, username, birthday, sex, address) VALUES
(OLD.id, OLD.username, OLD.birthday, OLD.sex, OLD.address);
END //
DELIMITER ;
DELETE FROM user WHERE id=38;
SELECT * FROM user_history;
+----+----------+------------+------+---------+
| id | username | birthday   | sex  | address |
+----+----------+------------+------+---------+
| 38 | name1    | 1995-05-10 | 1    | ***     |
+----+----------+------------+------+---------+
1 row in set (0.00 sec)
```

## 游标

将检索出来的所有的数据集合先保存在内存中然后依次取出每条数据进行处理。游标中有个指针的概念，指针指明了当前行记录的地址信息，在游标的处理过程中通过移动指针进行逐行读取数据。

# 分布式架构

## 主从复制

将主数据库中的DDL和DML操作通过**二进制日志binlog**传输到从数据库上，然后将这些日志重新执行，从而使得从数据库的数据与主数据库保持一致。

**主从复制的作用**

- 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份
- 负载均衡：降低单个服务器的压力
- 高可用和故障切换：帮助应用程序避免单点失败
- 升级测试：可以用更高版本的MySQL作为从库
- 主数据库出现问题，可以切换到从数据库。
- 读写分离，主数据库用于写，从用于读。
- 可以在从数据库上进行日常备份

**工作原理**

主从复制的实现过程如下图所示：


1. master的binlog线程在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中（不一定，根据binlog刷盘的策略决定），一直追加到文件中，大小超过一定容量拆分。
2. salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。即负责从master上拉取 binlog 内容，放进自己的relay log中。
3. slave的执行线程SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。

**半同步复制**

上述的介绍的实现是**异步复制**，异步复制存在问题，当主数据库宕机binlog还没有来得及被从接收，这时候将从切换为主，数据丢失不是最新的，存在一致性问题。

为解决引入**半同步复制**，在异步中引入一个同步机制，主在提交事务前，将数据的更新写入binlog，从数据库将binlog写入relay日志成功后，主才去提交事务。当超时后，会降级为异步服务。半同步适用于，对于数据一致性要求高，主从的网络延迟低的系统。

异步复制和半同步复制是mysql实时同步的解决方案，如下图异步复制和半同步复制的处理逻辑区别。


## 读写分离

读写分离是依赖于主从复制。实现方案如下：

1、使用mysql-proxy代理

优点：mysql内置功能直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用

缺点：降低性能， 不支持事务

2、使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。

如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin插件拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。

3、使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务。

缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。

4、gorm实现读写分离：

参考：https://cloud.tencent.com/developer/article/1773989

## 备份恢复

1、备份计划

视库的大小来定，一般来说 100G 内的库，可以考虑使用 mysqldump 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump 备份出来的文件比较小，压缩之后更小)。

100G 以上的库，可以考虑用 xtranbackup 来做，备份速度明显要比 mysqldump 要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。

2、备份恢复失败如何处理

首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。

3、mysqldump和xtrabackup实现原理

mysqldump：

**mysqldump 属于逻辑备份。**加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session 的事务隔离级别为 RR，之后显式开启一个事务，这样就保证了该事务里读到的数据都是执行事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁，等开启事务后，再记录下数据库此时 binlog 的位置(show master status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务

Xtrabackup:

**xtrabackup 属于物理备份**，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成 innodb 的备份后，会做一个 flush engine logs 的操作，确保所有的 redo log 都已经落盘(涉及到事务的两阶段提交概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事情)。然后还需要 flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。

## 主从切换

。。。

## 分库分表

分库分表就是将数据库、表不仅拆分，而且拆分到不同机器上。一个表有多份，可以指定一张表的shardKey，然后对shardKey取hash，根据hash值将数据放到不同的数据库中。这个可以解决单机物理资源的瓶颈问题。按照业务分为不同的数据库，数据库存在多个副本分布在不同的物理节点上。分为垂直切分和水平切分，不同的切分方式有不同的优缺点和适用场景。分库分表的示例如下。

上面示例先根据业务耦合性垂直分库，然后再针对单个库进行水平分表。

**优点**

1. 不存在单库数据量过大、高并发的性能瓶颈，提升系统稳定性和负载能力
2. 应用端改造较小，不需要拆分业务模块

**缺点**

1. 跨分片的事务一致性较难保障，一般需要一层中间件，介于业务和db之间。分布式事务。
2. 跨库的join关联查询性能较差
3. 跨分片的排序问题
4. 分布式唯一id

### 问题

分库分表能有效地缓解单机和单库带来的性能瓶颈和压力，突破网络IO、磁盘存储、CPU处理能力的瓶颈，同时也带来了一些问题。能用读写分离，sql索引优化解决的就不去分库分表，**当单表的数据量达到千万级别，或者物理存储达到1t，**需要去考虑分库分表。

**1、事务一致性问题**

当更新内容同时分布在不同库中，不可避免会带来跨库事务问题。跨分片事务也是分布式事务，没有简单的方案，一般可使用”XA协议”（详见下文分布式事务）和”两阶段提交”处理。

分布式事务能最大限度保证数据库操作的原子性。但在提交事务时需要协调多个节点，推后了提交事务的时间点，延长了事务的执行时间。导致事务在访问共享资源时发生冲突或死锁的概率增高。随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平扩展的枷锁。

**2、跨节点关联查询 join 问题**

切分之前，系统中很多表和详情页所需的数据可以通过sql join来完成。而切分之后，数据可能分布在不同的节点上，此时join带来的问题就比较麻烦了，考虑到性能，尽量避免使用join查询。

解决这个问题的一些方法：

a. 全局表：

全局表，也可看做是”数据字典表”，就是系统中所有模块都可能依赖的一些表，为了避免跨库join查询，可以将这类表在每个数据库中都保存一份。这些数据通常很少会进行修改，所以也不担心一致性的问题。比如创建全局表。每个节点都有该表的全量数据，该表的所有操作都将广播到所有物理分片（set）中。

b. 字段冗余：

一种典型的反范式设计，利用空间换时间，为了性能而避免join查询。例如：订单表保存userId时候，也将userName冗余保存一份，这样查询订单详情时就不需要再去查询”买家user表”了。

但这种方法适用场景也有限，比较适用于依赖字段比较少的情况。而冗余字段的数据一致性也较难保证，就像上面订单表的例子，买家修改了userName后，是否需要在历史订单中同步更新呢？这也要结合实际业务场景进行考虑。

c. 数据组装：

在系统层面，分两次查询，第一次查询的结果集中找出所有的关联数据id，然后根据id发起第二次请求得到关联数据。最后将获得到的数据进行字段拼装。

**3、跨节点分页、排序、函数问题**

跨节点多库进行查询时，会出现limit分页、order by排序等问题。分页需要按照指定字段进行排序，当排序字段就是**分片字段**（数据切分时，将一个大表被分成若干个分片表，需要一定的规则，这样按照某种业务规则把数据分到某个分片的规则就是分片规则，分片规则中用的字段是分片字段），通过分片规则就比较容易定位到指定的分片节点。当排序字段非分片字段时，就变得比较复杂了。需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户。显然这个过程是会降低查询的效率。对IO，CPU也会增加额外的负担。如果取得页数很大，情况则变得复杂很多，因为各分片节点中的数据可能是随机的，为了排序的准确性，需要将所有节点的前N页数据都排序好做合并，最后再进行整体的排序，这样的操作是很耗费CPU和内存资源的，所以页数越大，系统的性能也会越差。在使用Max、Min、Sum、Count之类的函数进行计算的时候，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总、再次计算，最终将结果返回。

**4、全局主键避重问题**

在分库分表环境中，由于表中数据同时存在不同数据库中，主键平时使用的自增长将无用武之地，某个分区数据库自生成的ID无法保证全局唯一。因此需要单独设计全局主键，以避免跨库主键重复问题。解决方案：分布式id，详见下文。

### 切分规则

关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当单表的数据量达到1000万或100GB以后，由于查询维度较多，即使读写分离添加从库、优化索引，做很多操作时性能仍下降严重。分库分表就是按照一定的规则，对原有的数据库和表进行拆分，把原本存储于一个库的数据分块存储到多个库上，把原本存储于一个表的数据分块存储到多个表上。切分规则如下。

**1、查询切分**

首先数据库分片，将sharding key记录在一个单独的库中，你每次要查询数据库的时候，请先到mapping db里面去查一下你应该到那个数据库去拿数据。



**2、范围切分**

按照范围来切分，比如说按照时间范围和ID的范围来进行切分。例如：按日期将不同月甚至是日的数据分散到不同的库中；将userId为1 ~ 9999的记录分到第一个库，10000 ~ 20000的分到第二个库，以此类推。优点如下。

- 单表大小可控；
- 天然便于水平扩展，后期如果想对整个分片集群扩容时，只需要添加节点即可，无需对其他分片的数据进行迁移；
- 使用分片字段进行范围查找时，连续分片可快速定位分片进行快速查询，有效避免跨分片查询的问题。

缺点：热点数据成为性能瓶颈；连续分片可能存在数据热点，例如按时间字段分片，有些分片存储最近时间段内的数据，可能会被频繁的读写，而有些分片存储的历史数据，则很少被查询

**3、Hash取模切分**

一般按照hash(key) % N 这样来计算出需要存放的节点。其中 hash 函数是一个将字符串转换为正整数的哈希映射方法，N 就是数据库实例的数量。优点：数据分片相对比较均匀，不容易出现热点和并发访问的瓶颈。

缺点：

- 后期分片集群扩容时，需要迁移旧的数据（使用一致性hash算法能较好的避免这个问题）
- 容易面临跨分片查询的复杂问题。
- 如果频繁用到的查询条件中不带sharding key时，将会导致无法定位数据库，从而需要同时向多个库发起查询，再在内存中合并数据，取最小集返回给应用，分库反而成为拖累。

**4、一致性hash算法**

当我们在做数据库分库分表或者是分布式缓存时，不可避免的都会遇到一个问题：**如何将数据均匀的分散到各个节点中，并且尽量的在加减节点时能使受影响的数据最少**，包括上面的hash取模和一致性hash算法都是解决方案。

假设我们有 "semlinker"、"kakuqo"、"lolo"、"fer" 四个对象，分别简写为 o1、o2、o3 和 o4，然后使用哈希函数计算这个对象的 hash 值，值的范围是 [0, 2^32-1]。接着使用同样的哈希函数，我们将服务器也放置到哈希环上，可以选择服务器的 IP 或主机名作为键进行哈希，这样每台服务器就能确定其在哈希环上的位置。这里假设我们有 3 台缓存服务器，分别为 cs1、cs2 和 cs3。将对象和服务器都放置到同一个哈希环后，在哈希环上顺时针查找距离这个对象的 hash 值最近的机器，即是这个对象所属的机器。

**一致 Hash 算法是将节点和数据的哈希值构成一个环。数据的哈希值顺时针靠近哪个节点就存储在哪个节点上**。优点：当一个扩缩容时只会影响到少少部分的数据。缺点：当节点较少时会出现数据分布不均匀的情况。

## 分布式id

**UUID与数据库自增主键：**

因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据，按照顺序，**如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降**。在数据量大一些的情况下，用自增主键性能会好一些。但是缺点是分布式架构下自增id不一定全局唯一。

**分布式id解决方案：**

**I、uuid**

UUID是主键是最简单的方案，本地生成，性能高，没有网络耗时。

缺点：

- 由于UUID非常长，会占用大量的存储空间；
- 作为主键建立索引和基于索引进行查询时都会存在性能问题;
- 在InnoDB下，UUID的无序性会引起数据位置频繁变动，导致分页（数据库底层存储是磁盘分页存储的，跨页比较消耗性能）。

**II、雪花算法**

毫秒级时间戳+机器id+序列号

优点：

- 毫秒数在高位，生成的ID整体上按时间趋势递增；
- 不依赖第三方系统，稳定性和效率较高
- 并且整个分布式系统内不会产生ID碰撞；
- 可根据自身业务灵活分配bit位。

缺点：强依赖机器时钟，如果时钟回拨，则可能导致生成ID重复。

## 分布式事务

> 参考：https://juejin.cn/post/6844903647197806605

### 基础

- 分布式事务：事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。
- 一致性：强一致性、弱一致性、最终一致性
- CAP：Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得。
- BASE 理论指的是**基本可用Basically Available，软状态 Soft State，最终一致性 Eventual Consistency**，核心思想是即便无法做到强一致性，但应该采用适合的方式保证最终一致性。

分布式事务产生的场景：

一个是service产生多个节点，另一个是resource产生多个节点。

1、service多个节点

微服务架构下，用户的资产可能分为好多个部分，比如余额，积分，优惠券等等。无法保证积分扣减了之后，优惠券能否扣减成功。

![img](https://img-blog.csdnimg.cn/img_convert/cc95d39cef7ab73b9145cc1417313ff7.png)

2、resource多个节点

Mysql一般来说装千万级的数据就得进行分库分表，对于一个支付宝的转账业务来说，你给的朋友转钱，有可能你的数据库是在北京，而你的朋友的钱是存在上海，所以我们依然无法保证他们能同时成功。

### 解决方案

**1、两阶段提交XA**

两阶段提交，顾名思义就是要分两步提交。存在一个负责协调各个本地资源管理器的**事务管理器，本地资源管理器一般是由数据库实现**，事务管理器在第一阶段的时候询问各个资源管理器是否都就绪，如果收到每个资源的回复都是 yes，则在第二阶段提交事务，如果其中任意一个资源的回复是 no, 则回滚事务。

大致的流程：

1. 第一阶段（prepare）：事务管理器向所有本地资源管理器发起请求，询问是否是 ready 状态，所有参与者都将本事务能否成功的信息反馈发给协调者；
2. 第二阶段 (commit/rollback)：事务管理器根据所有本地资源管理器的反馈，通知所有本地资源管理器，步调一致地在所有分支上提交或者回滚。

存在的问题：

- 同步阻塞：当参与事务者存在占用公共资源的情况，其中一个占用了资源，其他事务参与者就只能阻塞等待资源释放，处于阻塞状态。
- 单点故障：一旦事务管理器出现故障，整个系统不可用
- 数据不一致：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，只有部分参与者提交了事务，使得系统数据不一致。
- 不确定性：当协事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。

**2、TCC**

TCC（Try-Confirm-Cancel），TCC 事务机制相比于上面介绍的 XA，解决了其几个缺点：

- 解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。
- 同步阻塞：引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。
- 数据一致性，有了补偿机制之后，由业务活动管理器控制一致性

阶段：

1. Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性）
2. Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作满足幂等性。要求具备幂等设计，Confirm 失败后需要进行重试。
3. Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源 Cancel 操作满足幂等性 Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致。

缺点：基于 TCC 实现分布式事务，会将原来只需要一个接口就可以实现的逻辑拆分为 Try、Confirm、Cancel 三个接口，所以代码实现**复杂度相对较高，应用较少**。

**3、本地消息表**

1. 当系统 A 被其他系统调用发生数据库表更操作，首先会更新数据库的业务表，其次会往相同数据库的**消息数据表**中插入一条数据，两个操作发生在同一个事务中
2. 系统 A 的脚本定期轮询本地消息往 mq 中写入一条消息，如果消息发送失败会进行重试
3. 系统 B 消费 mq 中的消息，并处理业务逻辑。如果本地事务处理失败，会在继续消费 mq 中的消息进行重试，如果业务上的失败，可以通知系统 A 进行回滚操作


本地消息表实现的条件：

- 消费者与生产者的接口都要支持幂等（因为需要进行重试机制）
- 生产者需要额外的创建消息表
- 需要提供补偿逻辑，如果消费者业务失败，需要生产者支持回滚操作

容错机制：

- 步骤 1 失败时，事务直接回滚
- 步骤 2、3 写 mq 与消费 mq 失败会进行重试
- 步骤 3 业务失败系统 B 向系统 A 发起事务回滚操作

此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。

应用：跨行转账可通过该方案实现。用户 A 向用户 B 发起转账，首先系统会扣掉用户 A 账户中的金额，将该转账消息写入消息表中，如果事务执行失败则转账失败，如果转账成功，系统中会有定时轮询消息表，往 mq 中写入转账消息，失败重试。mq 消息会被实时消费并往用户 B 中账户增加转账金额，执行失败会不断重试。

**4、可靠消息最终一致性**

1. A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作
2. 如果消息发送成功，则执行本地事务
3. 如果本地事务执行成功，则想 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息
4. B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求

目前市面上支持该方案的 mq 只有阿里的 rocketmq, 该方案应用场景也比较多，比如用户注册成功后发送邮件、电商系统给用户发送优惠券等需要保证最终一致性的场景。

# sql优化

**分析过程：**

慢查询日志可以通过配置开启，慢查询原因：查询条件没有命中索引；load了不需要的数据列；数据量太大。

1、首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。

```sql
select * from table where age > 20 limit 1000000,10 
--这种查询其实也是有可以优化的余地的，这条语句需要load 1000000数据然后基本上全部丢弃,只取10条当然比较慢,如下优化
select * from table where id in (select id from table where age > 20 limit 1000000,10)
```

2、分析语句的执行计划explain命令查看，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。

3、如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，单表数据量超过千万条，如果是的话可以进行横向或者纵向的分表。

**sql优化经验：**

https://blog.nowcoder.net/n/e98629d309234e2487165bc92a13a4f6?from=nowcoder_improve

1、对于慢查询加索引：

优先考虑where、order by、group by中使用到的字段；

避免出现select  *，使用具体的字段

2、尽量覆盖索引，避免全表扫描：

- 避免使用in 和not in 如果是连续值用between代替 ，如果是子查询用exists替代

```sql
  -- 不走索引
  select * from A where A.id in (select id from B);
  -- 走索引
  select * from A where exists (select * from B where B.id = A.id);
```

- 避免使用or 用union代替，

- 避免进行null值的判断 给字段添加默认值再判断，

- 避免在字段开头模糊查询%b 在字段后面使用模糊查询a% ，

- 避免在where条件中等号的左侧进行表达式、函数操作

- 使用索引列作为条件进行查询时，需要避免使用<>或者!=等判断条件。 


# 安全

## sql注入

将SQL命令插入到Web表单递交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。恶意拼接的字符串，导致内部的sql查询逻辑失效。

防止SQL注入的解决方法如下：

- 对字符进行转义。
- 使用prepareStatement，具有占位符。
- 对用户的输入进行校验，可以通过正则表达式，或限制长度。
- 不要使用动态拼装sql，可以使用参数化的sql或者直接使用存储过程进行数据查询存取。
- 不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接。
- 不要把机密信息直接存放，加密或者hash掉密码和敏感的信息。
- 应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装
- sql注入的检测方法一般采取辅助软件工具或网站平台来检测

# 应用配置

**wait_timeout**

wait_timeout：超过该时间的空闲长连接，mysql会把该连接自动关闭。

mysql默认wait_timeout 28800 8小时
